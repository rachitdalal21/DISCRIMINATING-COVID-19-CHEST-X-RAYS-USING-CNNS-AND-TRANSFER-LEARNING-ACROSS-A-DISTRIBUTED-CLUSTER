{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x123216630>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # in_channels = 1 because they are greyscale images\n",
    "        # out_channels = 6 means, we're using 6, 5*5 filters/kernals, thus 6 outputs will be there\n",
    "        # output of the previous layer is the input to the next layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        # when moving from the convolutional layer to fully connected layers, inputs should be flattened\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        # out_features = 15 because we have 15 class labels\n",
    "        self.out = nn.Linear(in_features=60, out_features=15)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t    # here we show this for clarity\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1,12 * 4 * 4)   # change the shape accordingly\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        # softmax returns a probability of predictions for each class, \n",
    "        # however, we don't need this, if we're using cross_entropy during training\n",
    "        # t = F.softmax(t, dim=1) \n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( train_set, batch_size= 1 )\n",
    "batch = next(iter(data_loader)) # when training one batch we can use the iterator, otherwise a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0598, -0.1265,  0.1818,  0.1847,  0.0762],\n",
       "          [ 0.1003,  0.0602,  0.0628,  0.0830,  0.1645],\n",
       "          [ 0.0725, -0.1323, -0.0140,  0.0700,  0.1965],\n",
       "          [-0.1832,  0.1262, -0.1030,  0.1608,  0.0534],\n",
       "          [ 0.0628,  0.0274,  0.0783, -0.0811, -0.1549]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0769,  0.1114,  0.1558, -0.0901, -0.0383],\n",
       "          [ 0.1951, -0.1368, -0.0052, -0.1501, -0.1664],\n",
       "          [-0.1326, -0.1543,  0.0170, -0.1380,  0.0826],\n",
       "          [-0.0976, -0.0695,  0.0243, -0.0447,  0.1989],\n",
       "          [ 0.0046, -0.0609,  0.0042,  0.0111,  0.1765]]],\n",
       "\n",
       "\n",
       "        [[[-0.0508,  0.0094,  0.1131,  0.0586, -0.1609],\n",
       "          [ 0.1771, -0.0782, -0.1880,  0.0978, -0.0349],\n",
       "          [ 0.1403, -0.0576, -0.1824,  0.0245, -0.0202],\n",
       "          [-0.0708,  0.0675, -0.0774,  0.1668, -0.0570],\n",
       "          [ 0.0740,  0.0649, -0.1821,  0.1563, -0.1224]]],\n",
       "\n",
       "\n",
       "        [[[-0.0503,  0.1237,  0.1543, -0.0422, -0.0348],\n",
       "          [ 0.0908, -0.0557,  0.1802, -0.1077,  0.0588],\n",
       "          [ 0.1332,  0.0429, -0.1976, -0.0943, -0.1867],\n",
       "          [-0.1615, -0.0539,  0.0689,  0.1492, -0.1211],\n",
       "          [-0.1212, -0.1366, -0.0688,  0.1967,  0.0811]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0872,  0.1438, -0.1081,  0.1996, -0.1476],\n",
       "          [-0.0318,  0.0993, -0.1129,  0.1948, -0.1284],\n",
       "          [-0.0517,  0.0899,  0.1642,  0.0071, -0.1581],\n",
       "          [ 0.0634, -0.1802,  0.1746, -0.1945, -0.0063],\n",
       "          [-0.0709, -0.0547, -0.0592,  0.0347,  0.1387]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0149,  0.0698,  0.0107, -0.1441,  0.1827],\n",
       "          [ 0.1039,  0.0863, -0.0917,  0.0902,  0.1758],\n",
       "          [-0.0335, -0.1342,  0.0235,  0.0010,  0.1021],\n",
       "          [-0.0152, -0.1210, -0.1141, -0.0593,  0.0476],\n",
       "          [-0.0606, -0.1813, -0.0534, -0.1093,  0.1538]]]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarly we can see the weights of each convolutional layer \n",
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0368,  0.0192, -0.0093,  ...,  0.0636, -0.0106, -0.0501],\n",
       "        [-0.0602, -0.0193, -0.0103,  ...,  0.0406,  0.0144,  0.0573],\n",
       "        [ 0.0554,  0.0357,  0.0171,  ..., -0.0609, -0.0555, -0.0388],\n",
       "        ...,\n",
       "        [ 0.0294, -0.0610,  0.0533,  ..., -0.0540,  0.0311, -0.0063],\n",
       "        [-0.0162,  0.0026,  0.0236,  ..., -0.0409,  0.0549, -0.0037],\n",
       "        [ 0.0651, -0.0014,  0.0487,  ...,  0.0060,  0.0349,  0.0360]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarly we can see weights of fully connected layers\n",
    "network.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t\t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t\t torch.Size([6])\n",
      "conv2.weight \t\t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t\t torch.Size([12])\n",
      "fc1.weight \t\t torch.Size([120, 192])\n",
      "fc1.bias \t\t torch.Size([120])\n",
      "fc2.weight \t\t torch.Size([60, 120])\n",
      "fc2.bias \t\t torch.Size([60])\n",
      "out.weight \t\t torch.Size([10, 60])\n",
      "out.bias \t\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# can use network.parameters() as well, but the it doesn't give the names\n",
    "for name,param in network.named_parameters():\n",
    "    print(name ,\"\\t\\t\" , param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO need to create the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "\n",
    "'''\n",
    "b_size = Batch size for DataLoader\n",
    "'''\n",
    "def train_model(b_size, train_set):\n",
    "    \n",
    "    # here train_set should be tensors with images and label\n",
    "    train_loader = torch.utils.data.DataLoader( train_set, batch_size= b_size )\n",
    "    optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "    print(\"*****Batch size is :\", b_size, \"******\")\n",
    "    \n",
    "    for epoch in range(10):\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch in train_loader: # get batch\n",
    "            images, labels = batch\n",
    "\n",
    "            # train\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)  #since we're using cross_entropy, no need to use softmax in the forward function\n",
    "\n",
    "            # We need to clear them out before each instance\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  #calculate gradient\n",
    "            optimizer.step() #update weights\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "        print(\"epoch\", epoch, \"total_correct:\", total_correct, \"loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_of_correct_preds(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()    # because we don't need this function to track gradients\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "            (all_preds, preds)\n",
    "            ,dim=0\n",
    "        )\n",
    "        \n",
    "    return all_preds\n",
    "\n",
    "def new_eval(model, loader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        predicted = []\n",
    "        for batch in loader:\n",
    "            images, labels = batch\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "\n",
    "            y_test = test_label_batch.numpy()\n",
    "            predicted.append(predict)\n",
    "\n",
    "            print(\"Accuracy: \", accuracy_score(predicted, y_test))\n",
    "            print(\"Precision: \", precision_score(predicted, y_test, average='weighted'))\n",
    "            print(\"Recall: \", recall_score(predicted, y_test, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training confusion matrix\n",
    "\n",
    "data_loader = torch.nn.data.DataLoader(test_set, batch_size= 1)  #because we need to create one batch with all data\n",
    "\n",
    "train_preds = get_all_preds(network, data_loader)\n",
    "\n",
    "# train_set.targets   # target labels of train_set\n",
    "\n",
    "# predict\n",
    "train_preds.argmax(dim=1)   # gives the index with highest probability\n",
    "\n",
    "train_preds.argmax(dim=1).eq(train_set.targets) # returns binary values by comparing indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True, False])\n",
      "tensor(0.3333)\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([0, 1, 2])  #actual\n",
    "t2 = torch.tensor([0, 0, 1])  #predicted\n",
    "\n",
    "y = (t1 == t2)\n",
    "print(y)\n",
    "x = (t1 == t2).sum().float() / len(t1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1., 2., 3.]), tensor([4., 5., 6.])]\n"
     ]
    }
   ],
   "source": [
    "t1= torch.tensor([1,2,3], dtype=torch.float32)\n",
    "t2 = torch.tensor([4,5,6], dtype=torch.float32)\n",
    "l1 = [t1,t2]\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5., 6.])\n",
      "tensor([4., 5., 6., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.tensor([],dtype=torch.float32)\n",
    "t4 = torch.tensor([],dtype=torch.float32)\n",
    "l2 = [t2, t2]\n",
    "\n",
    "for td, te in l1, l2:\n",
    "    t3 = torch.cat(\n",
    "            (t3, td), dim=0\n",
    "        )\n",
    "    t4 = torch.cat(\n",
    "         (t4, te), dim=0\n",
    "        )\n",
    "           \n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
